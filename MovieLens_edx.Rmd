---
title: "HarvardX: PH125.9x Data Science  \n   Movie Recommendation System"
author: "Marc Bender"
date: "May 12, 2020"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "figure/")
knitr::opts_chunk$set(fig.show = "hold")
```


# Introduction
Movie recommendation systems are a powerful approach to predict how a given user _u_ will rate a movie _i_ based on different features. These recommendation systems are used by famous streaming services like Netflix. Indeed, Netflix initiated a competition in 2006 offering $1,000,000 to any person or team which is able to improve Netflix' own recommendation algorithm by at least 10 %. This prize was awarded in 2009 to the winning team _BellKor's Pragmatic Chaos_ achieving a 10.06 % improvement [1].

In this project we attempt to develop our own movie recommendation system with the publicly available MovieLens 10M dataset provided by the research group GroupLens. This dataset is a database with approximately 10 million ratings for over 10,000 movies from around 70,000 users and is used to train a machine learning algorithm utilizing the features in the edx subset to predict the outcome (movie ratings ranging from 0.5 to 5 stars with 0.5 star intervals) in the validation set (edx and validation set are provided by the staff; see 1.1). 

The required metric to assess model performance is the Root Mean Square Error (RMSE). RMSE is defined as the standard deviation of the residuals (prediction errors) and is widely used to measure differences between observed and predicted values in modelling approaches. As the RMSE measures squared distances it is sensitive to outliers, which has to be accounted for. This can be achieved by regularization approaches penalizing estimates generated by a small number of observations (detailed explanation in section 2.2). 
The RMSE is calculated as follows: 

$$RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i}(y_{u,i}-\hat{y}_{u,i})^{2}}$$

where $y_{u,i}$ represents an observed value in the test set for user _u_ and movie _i_, $\hat{y}_{u,i}$ represents the respective expected value and _N_ represents the number of observations. It can simply be calculated in R using a function such as: 

```{r Define RMSE function}
# define function to calculate RMSE
RMSE <- function(true_ratings, predicted_ratings){
sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

provided by Prof. Irizarry during the HarvardX: PH125.8x machine learning class [2]. A lower RMSE corresponds to an overall better model performance, therefore the aim in machine learning is to minimize this error term. For this course an RMSE < 0.86490 shall be achieved.

The modelling approach was conducted on a standard home laptop with limitations concerning computational capacity (just 8 GB RAM). Therefore algorithms like _lm_ or _knn_ were not feasible and an approach calculating least square estimates manually has been chosen. 

After loading the dataset, different variables were analysed in respect to effects on the rating of users. Models incorporating these effects into the model term were generated and tuned (cross-validation by splitting edx into training and test set) and the best performing model was chosen.This final model was retrained on the whole edx set and validated with the retained data from the validation set. 

## Data ingestion

The code to load the dataset was provided by the staff. Some adjustments prior to splitting the dataset into edx and validation subsets have been made. These changes include extracting the release year of a movie from the title column, transforming the timestamp into a date (rounded to the respective week), calculating the age of movies by substracting the release year from 2009 (data was gathered until 2008) and calculating the Rate as number of ratings per year for each movie. Algorithm development was carried out solely on the edx subset and the final model was independently validated at the end by applying the algorithm to the validation subset and comparing observed with predicted values within said subset.

```{r Load Dataset, message=FALSE, warning= FALSE}
#load required packages
library(Hmisc)
library(tidyverse)
library(data.table)
library(caret)
library(doParallel)
library(lubridate)
library(kableExtra)

#activate parallel computing to avoid CPU-bound limitations
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# as of R 4.0.0 stringsAsFactors in as.data.frame has to be set to TRUE 
movies <- as.data.frame(movies, stringsAsFactors =TRUE) %>%
  mutate(movieId = as.numeric(levels(movieId))[movieId],
         title = as.character(title),
         genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

#### Transformation of the movielens dataset
movielens_trans <- movielens %>%
  # extract year from title column
  mutate(year = as.numeric(str_sub(title,-5,-2)))%>%
  # calculate age of movies
  mutate(age = 2009-year) %>%
  # change timestamp to date (week format)
  mutate(date = as_datetime(timestamp)) %>%
  mutate(date = round_date(date, unit = "week"))

# add average number of ratings/year to each movie
movielens_trans <- movielens_trans %>%
    # calculate average number of ratings/year
  group_by(movieId) %>%
  summarize(n = n(), years = 2009 - first(year),
            title = title[1],
            rating = mean(rating)) %>%
  mutate(rate = n/years) %>%
    # add Rate to the existing edx data
  select(movieId, rate) %>%
  right_join(movielens_trans,by="movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens_trans[-test_index,]
temp <- movielens_trans[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, movielens_trans, removed)

```







\pagebreak
# Methods

## Exploratory data analysis and data visualisation

Exploratory data analysis is an important step in machine learning as it allows us to familiarize ourselves with the structure of the dataset, parameters that might be useful for modeling and general trends within the data. To get a quick glance of the data we can inspect the data and view the first 6 rows with the _head()_ command. As shown below, the edx subset contains 10 variables: _movieId_, _rate_, _userId_, _rating_, _timestamp_, _title_, _genres_, _year_, _age_ and _date_. 


```{r, echo = FALSE}
# view first rows of the data
head(edx)
```


A summary of the data shows that the edx subset contains 9,000,055 observations, the mean rating $\mu$ is 3.512 and there are no missing values in the dataset. The mean rating $\mu$ will be saved for later and is depicted by a red dashed line in subsequent plots.


```{r, include =FALSE}
# mean rating
mu <- edx %>% 
  summarize(mean = mean(rating)) %>% pull(mean)
```


```{r, echo = FALSE}
# summary
summary(edx)
```


The dataset contains 10,677 unique movies in 797 unique genres rated by 69,878 unique users. 


```{r, echo = FALSE}
# number of unique movies, genres and users
edx %>%
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId),
            n_genres = n_distinct(genres)) %>% 
  kable(format = 'markdown')
```

\pagebreak
In order to analyze the parameters in the dataset it is crucial to inspect the data distribution of the respective parameters.A histogram of movie ratings for instance reveals a left skewed distribution (users tend to give positive ratings more often than negative ones) and a higher frequency of full-star ratings compared to half-star ratings. The most frequent rating is 4 stars, followed by 3 and 5 stars. The least frequent rating is 0.5 stars. 


```{r}
# distribution of movie ratings
edx %>%
  group_by(rating) %>%
  ggplot(aes(rating)) + 
  geom_histogram(bins = 30, color = "black") +
  theme_bw() +
  scale_y_continuous(breaks = seq(0,2500000,by=500000)) +
  ggtitle("Rating distribution")
```


\pagebreak
### Movie Effect

To analyze the data for a putative movie effect, a histogram depicting the number of ratings per movie and a histogram showing the average rating per movie were generated.
Some movies were rated over 10,000 times whereas other movies were rated very few times, sometimes even just once leading to unstable representations of the true movie rating and large errors. Furthermore some movies were generally rated higher (good movies) and some were generally rated lower (bad movies) confirming a suggested movie effect.  


```{r}

# distribution of number of ratings per movieId 
edx %>%
  count(movieId) %>%
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") +
  scale_x_log10()+
  theme_bw() +
  xlab("Number of ratings") +
  ylab("Number of movies") +
  ggtitle("Distribution of number of ratings per movie")
```

\pagebreak
```{r}

# movie effect
edx %>% 
  group_by(movieId) %>% 
  summarize(b_movie = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_movie)) + 
  geom_histogram(bins = 30, color = "black") +
  geom_vline(xintercept = mu, color = "red", lty = 2) +
  theme_bw() +
  xlab("Average rating") +
  ylab("Number of movies") +
  ggtitle("Distribution of average rating per movie")
```

\pagebreak
### User Effect

Analogously to the movie effect, differences in the amount of ratings per user, ranging from over 1,000 to as low as 10 ratings per user could be detected. On top of that the average rating distribution reveals a user bias as some users are very critical and some users seem to enjoy every movie.  


```{r User Effect}

# distribution of userIds 
edx %>%
  count(userId) %>%
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") +
  scale_x_log10()+
  theme_bw() +
  xlab("Number of ratings") +
  ylab("Number of users") +
  ggtitle("Number of ratings per user")
```

\pagebreak
```{r}
# User effect
edx %>% 
  group_by(userId) %>% 
  summarize(b_user = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_user)) + 
  geom_histogram(bins = 30, color = "black") +
  geom_vline(xintercept = mu, color = "red", lty = 2)+
  theme_bw() +
  xlab("Average rating") +
  ylab("Number of users") +
  ggtitle("Distribution of average movie rating per user")
```

\pagebreak
### Age Effect

The age of movies also affects the movie rating as more recent movies (age < 20 years) are rated below average with an increasing rating until the curve peaks at around 70 years (maximum rating of approximately 3.9 stars) and starts to converge towards the average again. This can be explained by a selection effect as old movies, which are still watched today, most likely are considered good movies. For recent movies this selection effect is not yet completed and newly published movies are watched by a bigger portion of the population therefore rated closer to the average.


```{r, message = FALSE}

# Age effect
edx %>% group_by(age) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(age, rating)) +
  geom_point() +
  geom_smooth()  + 
  geom_hline(yintercept = mu, lty =2,color="red") +
  theme_bw() +
  scale_x_continuous(breaks = seq(0,100, by=10)) +
  xlab("Age of movie (years)") +
  ylab("Rating") +
  ggtitle("Effect of movie age on rating")
```

\pagebreak
### Rate Effect

Another possible effect influencing movie ratings is the number of ratings per year for each movie. To explore this putative effect we can take a look at the most and least rated movies as shown in the following tables. The depicted ratings are the averages over all users that rated the respective movie. 

```{r, message = FALSE}
#### Rate
# check for movies with the highest number of ratings per year
most_rated <- edx %>%
  group_by(title) %>%
  summarize(mean_rate = mean(rate),
            mean_rating= mean(rating),n = n()) %>%
  top_n(25, mean_rate) %>%
  arrange(desc(mean_rate)) 

# print table
most_rated %>% kable(format = 'markdown')
```

\pagebreak
```{r, message = FALSE}
# check for movies with the lowest number of ratings per year
least_rated <- edx %>%
  group_by(title) %>%
  summarize(mean_rate = mean(rate),
            mean_rating= mean(rating), n=n()) %>%
  top_n(-25, mean_rate) %>%
  arrange(desc(mean_rate)) 

# print table
least_rated %>% kable(format = 'markdown')
```

The 25 movies with the highest number of ratings per year seem to be generally rated higher than average with a lot of 4-star ratings and only 1 movie rating lower than 3.5 stars. The least rated movies on the other hand seem to have a higher variance and are rated in a range from 1.5 to 5 stars. This high variance is explained by the low number of ratings (1 rating for each movie except for the movie "The Bell Boy"). 

Indeed, the 25 movies with the highest number of ratings per year have an average rating of 4.05 stars, whereas the least rated movies have an average rating as low as 3.14. 

```{r}
# compare average rating of most 25 and least 25 rated movies
data.frame(most_rated = mean(most_rated$mean_rating), 
           least_rated =mean(least_rated$mean_rating))
```

This trend is confirmed for the whole population by plotting the rating versus the rate (number of ratings per year) which generally shows that movies with more than ~600 ratings per year are ranked above average as they most likely represent popular movies and movies with fewer ratings per year are ranked below average as these are rather obscure movies. The plot also shows a higher variance of ratings for low rates as these ratings often represent the opinion of only a handful of individuals and are therefore representative for the whole population. 

```{r, message = FALSE}
# Plot rating of the movies versus the rate per year
edx %>% 
  group_by(movieId) %>%
  summarize(mean_rate = mean(rate),
            mean_rating = mean(rating)) %>%
  ggplot(aes(mean_rate, mean_rating)) +geom_point()+
  geom_smooth() + 
  geom_hline(yintercept = mu, lty =2,color="red") +
  theme_bw() +
  xlab("Rate (ratings/year)")+
  ylab("Average movie rating") +
  ggtitle("Average rating per movie vs. Ratings/year per movie")
```



\pagebreak
### Time Effect

Plotting rating versus the date of submission date of the rating reveals a time dependent effect of movie ratings. Until ~2001 users rated movies slightly above average; afterwards users were more critical and gave below average ratings. At the end of the data gathering period (2008), the trend converged towards the average and users started to rate movies above average again. These changes look like dynamic fluctuations around the average and might be due to differences how the population perceives movies in general at a given time. It should also be noted that this time data was only gathered for the period of 1995-2008.

```{r, message = FALSE}

#### Time Effect 
# Plot showing bias based on when the data was submitted
edx %>% 
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth() + 
  geom_hline(yintercept = mu, lty =2,color="red") +
  theme_bw()+
  xlab("Average rating") +
  ylab("Week of rating submission") +
  ggtitle("Fluctuations in average rating based on submission date")
```


\pagebreak
### Genre Effect

The last parameter that was analysed for an effect on movie ratings was the movie genre. The genre column is comprised of 797 unique genre combinations as previously explained. These combinations are formed by 19 distinct "base" genres such as Drama, Action, Comedy, etc. The following plot shows the average rating for a given "base" genre with error bars representing 95 % confidence intervals and indicates a genre effect with Horror movies ranking the lowest and movies of the Film-Noir genre ranking the highest.  


```{r}

#### Genre
# as str_split on the whole data.frame requires too much RAM this is a workaround
# to extract mean and 95% ci for each respective genre

# extract genre names
genre_names <- unique(edx$genres) %>%
  str_split("\\|") %>%
  unlist() %>%
  unique() %>%
  .[-length(.)]

# apply function over each genre to calculate mean and 95% ci
stat_genre <- sapply(genre_names, function(x){
edx %>% 
  filter(str_detect(edx$genres, x)) %>%
  # the confidence intervals could also be calculated by a bootstrap approach but were #
  # using way to much computation time. Although data is not normal, based on CLT this #
  # approximation of the cis should work well                                          #
  summarize(ci = list(mean_cl_normal(rating))) %>%
  unlist()
})%>%
  t() %>%
  data.frame() %>%
  rownames_to_column("Genre")

# plot average genre rating with 95% confidence intervals
stat_genre %>% mutate(Genre = reorder(Genre, ci.y)) %>%
    ggplot(aes(Genre, ci.y)) +
    geom_point(size=1) +
    geom_errorbar(aes(ymin = ci.ymin, ymax = ci.ymax),width=0.5)  +
    geom_hline(yintercept = mu, lty =2,color="red") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90)) +
    ylab("Average rating (+ 95% CI)") +
    ggtitle("Average rating by genre")

```


\pagebreak
## Modelling Approach

The aim of the modelling approach was to minimize the RMSE as described in the introduction. To compare modelling approaches to each other, a reference model needs to be definded. In this case this baseline model was the prediction of the average $\mu$ for each rating. Afterwards the different effects were added to analyze an improvement over this baseline model using the insights gained in the previous chapter. 

The exploratory data analysis demonstrated that there are a lot of movie ratings with only few observations, leading to inflated errors as squared distances were calculated. Therefore a regularization approach was applied to minimize the effect of these estimates generated by small sample sizes. Regularization shrinks errors of small sample sizes towards zero by adding a penalty term without significantly affecting observations obtained by large sample sizes and therefore yields a more robust and meaningful estimate. 


### Splitting edx into training and test set for model tuning

To perform cross-validation and define the best tuning parameters (if present in the respective model) the edx dataset was split into training and test set using the caret pipeline. 

```{r, message = FALSE, warning=FALSE}
# split edx data into training and test set for parameter tuning 
set.seed(12)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in test set are also in training set
test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test set back into training set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)

rm(temp, removed)
```

```{r, include=FALSE}
#calculate mean of train set for modeling purposes
mu <- train_set %>% 
  summarize(mean = mean(rating)) %>% pull(mean)
```

\pagebreak
### Baseline Model

The baseline model used as reference was to predict the average $\mu$ of the training set for each rating.
This model explains all differences by random variation: 

$$Y_{u, i} = \mu + \epsilon_{u, i}$$

where $\epsilon_{u,i}$ is the independent error from the same distribution centered at 0

Using this model a naive RMSE was obtained:  

```{r, message = FALSE, warning=FALSE}

# predict all values to be mean mu of training set 
mu_hat <- mu
naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse

# show results in table
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
rmse_results %>% kable(format = 'markdown')
```

\pagebreak
### Movie Model

In the data exploration section, differences in ratings for different movies could be observed, indicating a movie effect. The model incorporating this effect is:

$$Y_{u, i} = \mu + b_{i} + \epsilon_{u, i}$$

where $b_{i}$ is the movie effect or movie bias for each respective movie. A negative bias $b_{i}$ indicates a bad movie therefore ranking below average. A positive bias corresponds to a good movie ranking above average. Therefore this model better represents the "true" rating of each movie shown by an improvement of the RMSE over the baseline model:  


```{r}

# define model taking movie bias into account
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_movie = mean(rating - mu))

# prediction of ratings based on movie bias
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_movie

# calculate RMSE for movie model
model_2_rmse <- RMSE(test_set$rating, predicted_ratings)

# bind results to rmse table
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_2_rmse ))
rmse_results %>% kable(format = 'markdown')
```

\pagebreak
### Movie + User Model

Another insight gained during data exploration was the difference in ratings by user as some users tend to enjoy most movies and rate movies generally higher, whereas other users are more critical and rate movies generally lower than average. To account for these user specific differences the following model was developed:

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$

where $b_{u}$ is the user bias with a positive $b_{u}$ corresponding to a generous user and a negative $b_{u}$ indicating a critical user. The combination of movie and user bias greatly improved model performance: 


```{r}
#### 3.: User + Movie Model
# define model taking movie and user bias into account
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_user = mean(rating - mu - b_movie))

# prediction of ratings based on movie and user bias
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_movie + b_user) %>%
  .$pred

# calculate RMSE for movie model
model_3_rmse <- RMSE(test_set$rating, predicted_ratings)

# bind results to rmse table
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_3_rmse ))
rmse_results %>% kable(format = 'markdown')
```


### Regularized Movie + User Model

To further improve model performance, regularization was applied to minimize error inflation caused by small sample sizes. The regularisation formula for movie bias $b_i (\lambda)$ for a given $\lambda$ is:

$$b_i (\lambda) = \frac{1}{\lambda + n{i}} \sum (Y_i - \mu)$$

where $n_{i}$ is the number of ratings per movie _i_ and $\lambda$ is a hyperparameter that is object to tuning by a least square approach. Analogously the regularized user bias $b_u (\lambda)$ can be calculated as:

$$b_u(\lambda) = \frac{1}{\lambda + n_{u}}\sum(Y_i - (\mu + b_i(\lambda)))$$

using the same $\lambda$ as for the regularized movie bias with $n_{u}$ being the number of movies user _u_ rated. The best hyperparameter $\lambda$ was chosen by training the model using different values for $\lambda$ and chosing that value $\lambda$ which minimzes the RMSE on the test set. 

```{r}

# Regularized Movie and User Model 

# tuning hyperparameter lambda using training set
lambdas <- seq(0, 7, 0.25)

rmses <- sapply(lambdas,function(l){
  
  #Calculate the mean of ratings from the train_set training set
  mu <- mean(train_set$rating)
  
  #Adjust mean by movie effect and penalize low number on ratings
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  #ajdust mean by user and movie effect and penalize low number of ratings
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  #predict ratings in the training set to derive optimal penalty value 'lambda'
  predicted_ratings <-   test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(test_set$rating, predicted_ratings))
})

# plot rmses vs lambdas
data.frame(lambda = lambdas, rmse = rmses) %>% 
  ggplot(aes(lambda, rmse)) + 
  geom_point() +
  theme_bw() + 
  ylab("RMSE") + 
  ggtitle("Tuning of regularization parameter lambda")
```

The optimal value for $\lambda$ using this model was $\lambda$ = 5.25 resulting in an RMSE of 0.8655127:

```{r}
# extract best value lambda
model_4_lambda  <- lambdas[which.min(rmses)]

# extract RMSE of the model using the optimal lambda
model_4_rmse <- min(rmses)

# bind results to the RMSE table
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model",  
                                     RMSE = model_4_rmse ))
rmse_results %>% kable(format = 'markdown')

```

\pagebreak
### Regularized Movie + User + Age+ Genre+ Rate + Time Model

The last model tested, used the variables _movieId_, _userId_,  _genres_,  _age_, _rate_ and _date_ as data exploration suggested an influence on ratings by all of these parameters. On top of that regularization was applied to minimize small sample errors. The model is defined as follows:

$$Y_{u, i} = \mu + b_{i} + b_{u} + b_{g} + b_{a} + b_{r} + b_{t} +\epsilon_{u, i}$$

where $b_{g}$ is the genre bias: $\sum_{k=1}^{K} (X_{u,i} \beta_{k})$ (with $x_{u,i}^{k} = 1$ if $g_{u,i}$ is genre k), \newline
$b_{a}$ is the age of movie bias: $b_{a} = f(a_{i})$ (with $f(a_{i})$ a smooth function of $a_{i}$), \newline
$b_{r}$ is the rate (ratings/year) per movie: $b_{r} = f(r_{i})$ (with $f(r_{i})$ a smooth function of $r_{i}$) \newline
and $b_{t}$ is the date user _u_ rated movie _i_: $b_{t} = f(t_{u,i})$ (with $f(t_{u,i})$ a smooth function of t_{u,i}). 

The regularized effects were obtained as follows:

__Regularized genre effect__:
$$b_g(\lambda) = \frac{1}{\lambda + n_{g}}\sum(Y_i - (\mu + b_i(\lambda) + b_u(\lambda)))$$

where $n_{g}$ is the number of ratings per genre.

__Regularized age of movie effect__:
$$b_a(\lambda) = \frac{1}{\lambda + n_{a}}\sum(Y_i - (\mu + b_i(\lambda) + b_u(\lambda) + b_g(\lambda)))$$


where $n_{a}$ is the number of ratings per movie age.

__Regularized rate effect:__
$$b_r(\lambda) = \frac{1}{\lambda + n_{r}}\sum(Y_i - (\mu + b_i(\lambda) + b_u(\lambda) + b_g(\lambda) + b_a(\lambda)))$$

where $n_{r}$ is the number of ratings per rate.

__Regularized date of submission effect__:
$$b_t(\lambda) = \frac{1}{\lambda + n_{t}}\sum(Y_i - (\mu + b_i(\lambda) + b_u(\lambda) + b_g(\lambda) + b_a(\lambda) + b_r(\lambda)))$$

where $n_{t}$ is the number of ratings per submission date.

The hyperparameter $\lambda$ was tuned and the optimal $\lambda$ was chosen as described previously. 

```{r}

### Regularized Movie + User + Age+ Genre+ Rate + Time Effect Model
lambdas <- seq(0, 7, 0.25)


rmses <- sapply(lambdas,function(l){

  #Calculate the mean of ratings from the train_set training set
  mu <- mean(train_set$rating)

  #Adjust mean by movie effect and penalize low number on ratings
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))

  #Ajdust mean by user and movie effect and penalize low number of ratings
  b_u <- train_set %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  #Ajdust mean by genre, user and movie effect and penalize low number of ratings
  b_g <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+l))

  #Ajdust mean by age, genre, user and movie effect and penalize low number of ratings
  b_a <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    group_by(age) %>%
    summarize(b_a = sum(rating - b_i - b_u- b_g - mu)/(n()+l))

  #Ajdust mean by rate, time, age, genre, user and movie effect and penalize low number
  # of ratings
  b_r <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_a, by="age") %>%
    group_by(rate,movieId) %>%
    summarize(b_r = sum(rating - b_i - b_u- b_g - b_a- mu)/(n()+l))

  #Ajdust mean by time, age, genre, user and movie effect and penalize low number of ratings
  b_t <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_a, by="age") %>%
    left_join(b_r, by="movieId")  %>%
    group_by(date) %>%
    summarize(b_t = sum(rating - b_i - b_u- b_g - b_a - b_r - mu)/(n()+l))

  test_set %>%
    left_join(b_i, by="movieId") %>% filter(is.na(b_i)) %>% .$b_i %>% sum()

  #predict ratings in the training set to derive optimal penalty value 'lambda'
  predicted_ratings <- test_set %>%
    left_join(b_i, by="movieId") %>% mutate(b_i = ifelse(is.na(b_i),0,b_i)) %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_a, by="age") %>%
    left_join(b_r, by="movieId") %>%mutate(b_r = ifelse(is.na(b_r),0,b_r)) %>%
    left_join(b_t, by="date") %>%
    mutate(pred = mu + b_i + b_u + b_g +b_a+b_r+b_t) %>%
    .$pred

  return(RMSE(test_set$rating, predicted_ratings))
})


# plot rmses vs lambdas
data.frame(lambda = lambdas, rmse = rmses) %>% 
  ggplot(aes(lambda, rmse)) + 
  geom_point() +
  theme_bw() + 
  ylab("RMSE") + 
  ggtitle("Tuning of regularization parameter lambda")
```

Using this approach, the RMSE could be further minimized. Modelling approaches leaving out one or more of the parameters from the previous model were also tested but did not yield any better results and were therefore left out to increase lucidity.

```{r}
# extract optimal lambda
model_5_lambda <- lambdas[which.min(rmses)]

# extract RMSE of the model using the optimal lambda
model_5_rmse <- min(rmses)

# bind results to the RMSE table
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized  Movie + User + Genre + Age + Rate + Time Effect Model",  
                                     RMSE = model_5_rmse ))
rmse_results %>% kable(format = 'markdown')
```

\pagebreak
# Results 

The model using _movieId_, _userId_,  _genres_,  _age_, _rate_ and _date_ performed best on the training set. Thus, this model was retrained on the whole edx dataset with the optimal value of $\lambda$ = 5.5 determined by cross-validation. Subsequently this model was applied to the validation dataset to determine the out of sample error for this model. With an RMSE of 0.8634613 the final model improved predictions by 18.54 % compared to the baseline model. 


```{r}

# optimal tuning parameter lambda (acquired in the last paragraph of the methods section)
lambda <- model_5_lambda

# calculate RMSE on validation set
predicted_ratings <- sapply(lambda,function(l){
  
  #Calculate the mean of ratings from the train_set training set
  mu <- mean(edx$rating)
  
  #Adjust mean by movie effect and penalize low number on ratings
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  #Ajdust mean by user and movie effect and penalize low number of ratings
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  #Ajdust mean by genre, user and movie effect and penalize low number of ratings
  b_g <- edx %>% 
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+l))
  
  # Ajdust mean by age, genre, user and movie effect and penalize low number of ratings
  b_a <- edx %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    group_by(age) %>%
    summarize(b_a = sum(rating - b_i - b_u- b_g - mu)/(n()+l))
  
  # Ajdust mean by rate effect and penalize low number of ratings
  b_r <- edx %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_a, by="age") %>%
    group_by(rate,movieId) %>% 
    summarize(b_r = sum(rating - b_i - b_u- b_g - b_a- mu)/(n()+l))
  
  # Ajdust mean by time effect and penalize low number of ratings
  b_t <- edx %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_a, by="age") %>%
    left_join(b_r, by=c("rate","movieId"))%>%
    group_by(date) %>%
    summarize(b_t = sum(rating - b_i - b_u- b_g - b_a - b_r - mu)/(n()+l))
  
  #predict ratings in the validation set with the final value of lambda
  predicted_ratings <- validation %>% 
    left_join(b_i, by="movieId") %>%  #mutate(b_i = ifelse(is.na(b_i),0,b_i)) %>% 
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_a, by="age") %>%
    left_join(b_r, by="movieId") %>%  #mutate(b_r = ifelse(is.na(b_r),0,b_r)) %>%
    left_join(b_t, by="date") %>%
    mutate(pred = mu +  b_i + b_u + b_g + b_a+ b_r + b_t) %>%
    .$pred
  
   return(predicted_ratings)
})

#calculate RMSE
model_5_validation  <- RMSE(validation$rating, predicted_ratings)

# bind results to the RMSE table
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Validation: Regularized  Movie + User + Genre + 
                                     Age + Rate + Time Effect Model",  
                                     RMSE = model_5_validation))
rmse_results %>% kable(format = 'markdown')
```

```{r, include = FALSE}
# Improvement over baseline model
improvement <- (1 - rmse_results$RMSE[6]/rmse_results$RMSE[1]) *100 #percent
```

Furthermore the pearson correlation between predicted and observed ratings was calculated to analyze if the general movie taste could be correctly predicted. A correlation coefficient of 0.581 was obtained suggesting a strong correlation between predicted and observed ratings. This correlation and a linear relationship between the two variables is depicted for visualization purposes for the first 10,000 entries. 

```{r, message=FALSE}
# calculate correlation of predicted and observed ratings
corr <- cor(validation$rating, predicted_ratings) 
colnames(corr) <- "pearson correlation coefficent"
corr %>% round(3) %>% kable(format = 'markdown')

# plot correlation of predicted and observed ratings (for visualization purposes: only the first 10000 entries used)
validation[1:10000,] %>% 
  mutate(pred = predicted_ratings[1:10000]) %>% 
  ggplot(aes(rating, pred)) + geom_jitter() +
  geom_smooth(method="lm") +
  theme_bw()
```





\pagebreak
# Conclusion

The aim of this project was to develop an algorithm improving predictions over a baseline model (baseline model: suggesting the average rating $\mu$ and explaining differences by random variation) and achieving an RMSE < 0.86490. Incorporation of six different bias terms - using the features _movieId_, _userId_,  _genres_,  _age_, _rate_ and _date_ - into the baseline model combined with a regularization approach lead to an improvement of 18.54 % over the baseline model with a final RMSE of 0.8634613, thus fulfilling the requirements for a successful model regarding the class guidelines. 

Although the results still suggest a relatively large error concerning predictions of the acutal rating, correlation analysis revealed a strong correlation of 0.581 between predicted and observed ratings, indicating that the general movie taste can be predicted confidently. As the goal of a movie recommendation system is to suggest movies a user might enjoy it does not matter too much to not predict the actual rating correctly as long as the general direction is correct. 

This applies especially for very high or very low actual ratings as a rating of 0.5 or 1.5 is both considered very bad and a rating of 4 or 5 stars is considered great. However, the model might fail to suggest movies correctly if the actual rating is somewhere around 2.5 or 3 stars as this can be seen as a threshold between good and bad movies. 

In summary, the developed model fulfills the purpose to recommend movies based on different features in a satisfactory manner. The recommendation system, however, could be improved by application of different algorithms like _knn_ or neural networks, if the computational capacity was available. 



\pagebreak
# Appendix

## References
[1] https://www.netflixprize.com/ \newline
[2] https://courses.edx.org/courses/course-v1:HarvardX+PH125.8x+1T2020/course

## Used Packages 

* Hmisc
  + summary of useful functions for data analysis
* tidyverse
  + Uniform way to process, clean, transform, ..., data
* data.table
  + Handling big datasets
* caret
  + Provides machine learning pipelines
* doParallel
  + Allowing R to use all CPU cores available
* lubridate
  + Facilitates handling time data 
* kableExtra
  + Display of tables in the Report

## Dataset

The MovieLens data set is available at https://grouplens.org/datasets/movielens/10m/.
The R script, R markdown file and this report can be accessed at https://github.com/MBender1992/MovieLens_Edx.

\pagebreak
## Session Info

```{r, echo = FALSE}
sessioninfo::session_info()
```


